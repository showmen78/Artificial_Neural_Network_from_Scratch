{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n\n# Artificial Neural Network (Ann) from Scratch\n\n<font size= \"4\" color=\"black\"> In this project Artificial Neural Network (ANN) was writeen from scratch.</font>\n\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-20T05:56:02.156817Z","iopub.execute_input":"2023-06-20T05:56:02.157841Z","iopub.status.idle":"2023-06-20T05:56:02.164515Z","shell.execute_reply.started":"2023-06-20T05:56:02.157802Z","shell.execute_reply":"2023-06-20T05:56:02.163065Z"}}},{"cell_type":"markdown","source":"\n# Table of Content\n\n\n    \n #### [1. Importing Libraries](#1) \n\n #### [2. Activation Fucntions](#2)\n    \n\n\n","metadata":{}},{"cell_type":"markdown","source":"<a id =\"1\"> </a>\n# [Importing Libraries](#1)\n\n<font size=\"4\"> Library used in this project.\n    \n\n <h5 style=\"color:red\">1: Numpy  </h4>\n <h5 style=\"color:red\">2: Math  </h4>\n    \n   \n </font>","metadata":{}},{"cell_type":"code","source":"#importing libraries\n\nimport numpy as np\n#import random\n\nimport math","metadata":{"_uuid":"8571bf29-01bb-4333-ab54-d257aab05ef0","_cell_guid":"f97444e5-b840-4edd-9495-de1efac15242","jupyter":{"outputs_hidden":false},"collapsed":false,"execution":{"iopub.status.busy":"2023-06-21T19:23:44.872764Z","iopub.execute_input":"2023-06-21T19:23:44.873227Z","iopub.status.idle":"2023-06-21T19:23:44.911872Z","shell.execute_reply.started":"2023-06-21T19:23:44.873189Z","shell.execute_reply":"2023-06-21T19:23:44.910068Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"2\"> </a>\n\n\n\n# [Activation Functions And Their Derivatives](#2)\n\n<ul style=\"color:blue \">\n    <li> <p style=\"font-size:16px\">Relu </p> </li>\n    <li> <p style=\"font-size:16px\">Sigmoid </p> </li>\n    <li> <p style=\"font-size:16px\">Tanh </p> </li>\n    <li> <p style=\"font-size:16px\">Softmax </p> </li>\n\n</ul>\n        \n","metadata":{}},{"cell_type":"code","source":"#activation function\ndef relu(z):\n    return np.maximum(z,0)\n\ndef derivative_of_relu(z):\n    return np.array(z>0,dtype='float')\n\ndef sigmoid(z):\n    return 1/(1+np.exp(-z))\n\ndef derivative_of_sigmoid(z):\n    sg= sigmoid(z)\n    return sg*(1-sg)\n\ndef tanh(z):\n    return np.tanh(z)\n\ndef derivative_of_tanh(z):\n    return 1-np.power(tanh(z),2)\n\ndef softmax(z):\n    return np.exp(z)/np.sum(np.exp(z),axis=0)","metadata":{"_uuid":"e5bee2cb-01e6-4b70-852f-6f97a106ae9b","_cell_guid":"1bed6e84-e8a2-4015-bc87-e2d047bd3070","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-21T19:23:45.991991Z","iopub.execute_input":"2023-06-21T19:23:45.992381Z","iopub.status.idle":"2023-06-21T19:23:46.000074Z","shell.execute_reply.started":"2023-06-21T19:23:45.992352Z","shell.execute_reply":"2023-06-21T19:23:45.998922Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n<link href=\"https://fonts.googleapis.com/css2?family=Roboto&display=swap\" rel=\"stylesheet\">\n\n\n# Layer Class\n\n\n<p ><font face=\"arial\" size=\"3\">Perform Calculation for one Layer.</font></p>\n\n### Methods \n<ul style=\"color:blue \">\n    <li> <p style=\"font-size:16px\">forward_propagation (input): <span style=\"color:black\"> runs forward propagation for a single      layer </span> </p> </li>\n    \n    <li> <p style=\"font-size:16px\">forward_propagation (input): <span style=\"color:black\"> runs forward propagation for a single      layer </span> </p> </li>\n\n\n</ul>\n","metadata":{}},{"cell_type":"code","source":"class Layer:\n    def __init__(self,w_size,last_layer,activation):\n        '''\n            input:\n                w_size: a list [no of input node, no of output node]\n                last_layer: is this layer the last layer(True/False)\n                activation: string (activation used in this layer)\n                \n        '''\n        \n        #parameters of this layer\n        self.w= np.random.randn(w_size[1],w_size[0])\n        self.dw=[]\n        \n        self.b= np.zeros((w_size[1],1))\n        self.db=[]\n        \n        self.z=[]\n        self.dz=[]\n        \n        self.last_layer= last_layer # is this the last layer\n        \n        self.x=[]  #input of this layer\n        self.A= [] #output of this layer\n        \n        #activation used in this layer\n        self.activation=activation\n\n\n\n\n    def forward_propagation(self,A):\n        '''\n            input:\n                A: input of this layer\n\n\n            output:\n                self.A: The output of this layer( activation(self.z))\n        '''\n        self.x=A\n        self.z= np.array(self.w.dot(A)+ self.b)\n        self.A= np.array(eval(self.activation+'(self.z)'))\n\n        return self.A\n\n\n    def back_propagation(self,y,w_next,dz_next,batch_size):\n        '''\n            input:\n                \n                y:an array of actual output of shape (no of class,no of samples)\n                w_next: w parameter of the next layer(is not used in the last layer)\n                dz_next: dz parameter of the next layer(is not used in the last layer)\n                batch_size : batch size\n\n            output:\n                dw,dz : an array , value of dw and dz of this layer\n        '''\n        # for the last layer\n        if self.last_layer:\n            self.dz= np.array(self.A-y)\n            #in the last layer, no of samples may not be the same as batch size\n            batch_size= y.shape[1]\n            \n        # if not last layer\n        else:\n            #self.dz= self.np.array(w_next.T.dot(dz_next)) * derivative_of_sigmoid(self.z)\n            self.dz= np.array(w_next.T.dot(dz_next)) * eval('derivative_of_'+self.activation+'(self.z)')\n\n\n        #self.dw= (1/batch_size)*np.array(self.dz.dot(self.input.T))\n        self.dw= (1/batch_size)*np.array(self.dz.dot(self.x.T))\n        self.db= (1/batch_size)*np.sum(self.dz,axis=1,keepdims=True)\n\n        return np.array(self.w),np.array(self.dz)\n\n    def calculate_cost(self,y,_type):\n        '''\n            input:\n                batch_size: batch size\n                y: actual value of the output\n                _type: type of cost function (binary classification / multiclassification)\n\n            output: return the cost value\n            Note: only the last layer calls this method\n        '''\n        #for binary classification\n        if _type=='binary_classification':\n            return np.sum((y*np.log(self.A))+(1-y)*np.log(1-self.A))*(-1/y.shape[1]) \n\n        #for multiclassification problem\n        if _type=='multi_classification':\n            return np.sum(y*np.log(self.A))/(-1/y.shape[1]) \n\n    def update_parameters(self,lr):\n        '''\n            lr: learning rate\n            Note: Updates the parameter \n        '''\n        self.w = self.w- (lr*self.dw)\n        self.b= self.b- (lr*self.db)","metadata":{"_uuid":"db50aca3-97fd-4b5c-89cf-4f98a98c5247","_cell_guid":"d282c91d-9421-48d1-b643-a05fe1085c85","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-21T19:23:47.075402Z","iopub.execute_input":"2023-06-21T19:23:47.076230Z","iopub.status.idle":"2023-06-21T19:23:47.089195Z","shell.execute_reply.started":"2023-06-21T19:23:47.076195Z","shell.execute_reply":"2023-06-21T19:23:47.088121Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Ann:\n    def __init__(self,neurons,activation,_type='binary_classification'):\n        '''\n            input:\n                neurons: a list of integer , contains the number of neurons in each layer eg(10,12,10)\n                activation: a list containing the name of activations for each layer\n                _type: a string, binary classification or multi classification\n        '''\n        \n        self.neurons= neurons\n        self.activation= activation\n       \n        #contains all the layer objects\n        self.layers=[]\n        self._type= _type\n        \n        \n        \n    def train(self,_input,y_acc,batch_size,test_input=np.zeros((1,4)),test_label=np.zeros((1,4)),lr=0.1,iterations=1,show_acc=[True,True]):\n            '''\n                input:\n                    _input: a numpy array, contains the input features , shape(input_features,sample no)\n                    y_acc : an array of actual output, shape(no of class, sample no)\n                    test_input: an array , contains the test input features , shape(input_features,test sample no)\n                    test_label: an array , actual output of the test cases , shape(no of class,no of test sample)\n                    \n                    batch_size: an int, batch size for training\n                    lr : a float , learning rate\n                    iterations: an integer value, training iterations\n                    show_acc: a list of bolean value ,show accuracy of [train,test] after each iteration\n            '''\n\n            #if there is no layer then create layer objects at first\n\n            if len(self.layers)==0:\n                #first value of the self.neuron will be the row of input array.\n                self.neurons.insert(0,_input.shape[0])\n                \n                #creating layer objects\n                for n in range(1,len(self.neurons)):\n                    self.layers.append(Layer([self.neurons[n-1],self.neurons[n]],n== len(self.neurons)-1,self.activation[n-1]))\n\n            #stores the shape of the original input\n            _input_shape= [_input.shape[0],_input.shape[1]]\n            \n            #converting the _input and outputs to mini batches\n            _input = [_input[:,j*batch_size:(j*batch_size)+batch_size] for j in range(math.ceil(_input.shape[1]/batch_size))]      \n            y_acc= [y_acc[:,j*batch_size:(j*batch_size)+batch_size] for j in range(math.ceil(y_acc.shape[1]/batch_size))]\n            \n            \n            #start the training \n            for i in range(iterations):\n                cost =0 #contains the cost value of a whole iteration\n                #for each batch\n                for batch_x,batch_y in zip(_input,y_acc):\n                    #input of the first layer\n                    A = batch_x\n\n                    #forward propagation for all layers\n                    for layer in self.layers:\n                        A= layer.forward_propagation(A)\n                        \n                    #calculate and add cost\n                    cost += self.layers[-1].calculate_cost(batch_y,self._type)\n\n                    #back propagation for all layers\n                    w=dz=0\n                    for layer in self.layers[::-1]:\n                        w,dz= layer.back_propagation(batch_y,w,dz,batch_size)\n                        #update parameters\n                        layer.update_parameters(lr)\n                \n                # after each iteration\n                #print train accuracy\n                if show_acc[0]:\n                    print('iter:{} - train_cost:{} - train_acc:{} -'.format(i,round(cost,2),self.predict(_input,y_acc,batch_size,True)[0]),\n                        end=' ')\n                \n                #print test accuracy\n                if show_acc[1]:\n                    print('test_acc:{}'.format(self.predict(test_input,test_label,batch_size,False)[0]),end='')\n                print() #draw a new line\n\n\n\n    def predict(self,x,y,batch_size,train=False):\n\n        '''\n            input:\n                x: an array of input features\n                y_acc: an array of actual outputs\n                batch_size: batch size\n                train: is this training data or testing data (True for training data)\n\n            output:\n                return a list , [accuracy,predicted output]\n        '''\n        \n        \n        #divide only the test data into mini batches, train data are divided into batches in the train function\n        if train==False:\n            x = [x[:,j*batch_size:(j*batch_size)+batch_size] for j in range(math.ceil(x.shape[1]/batch_size))]\n            y= [y[:,j*batch_size:(j*batch_size)+batch_size] for j in range(math.ceil(y.shape[1]/batch_size))]\n        #contains the accuracy\n        acc=[]\n        #contains the predicted output\n        y_pred=[]\n        predicted_output= np.zeros((1,1))\n        #counts how many samples are there\n        m=0 \n        #run forward propagation\n        for batch_x ,batch_y in zip(x,y):\n            a= batch_x\n            for l in range(len(self.layers)):\n                a= self.layers[l].forward_propagation(a)\n            \n            if np.sum(predicted_output)==0:\n                predicted_output=a\n                \n            else:\n                predicted_output= np.column_stack((predicted_output, a))\n            \n            if self._type== 'multi_classification':\n                a= np.argmax(a,0)\n                y_pred= np.argmax(batch_y,0)\n                m+= a.shape[0]\n                \n            if self._type == \"binary_classification\":\n                a=np.array(a>.5,dtype='float')\n                y_pred= batch_y\n                m+= a.shape[1]\n                \n\n            #m += a.shape[0]  #add the sample number in each batch to calculate avg accuracy\n            acc.append(np.sum(np.array(a==y_pred,dtype='float')))\n\n    \n        return [round(100*sum(acc)/m,2),predicted_output]\n       ","metadata":{"_uuid":"25bb8192-0641-44e0-9533-1a7e0c26c4ce","_cell_guid":"dafcc572-cc51-4a96-87d9-ba786a344fc4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-21T19:40:41.113767Z","iopub.execute_input":"2023-06-21T19:40:41.114188Z","iopub.status.idle":"2023-06-21T19:40:41.135871Z","shell.execute_reply.started":"2023-06-21T19:40:41.114157Z","shell.execute_reply":"2023-06-21T19:40:41.134784Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# x= [[1,2,4],[5,6,7]]\n# x= np.array(x).reshape(2,3)\n\n# y=[[1,0,1],[0,1,0],[0,0,0],[0,0,0]]\n\n# y= np.array(y).reshape(4,3)\n\n# a= Ann([4],['softmax'],'multi_classification')\n# a.train(x,y,2,x,y,.1,1,[True,True])","metadata":{"_uuid":"e831765b-34f0-4a7b-b080-a03ec598fc18","_cell_guid":"87c2de06-e450-4e8b-abf6-54037814e5ad","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-06-21T19:47:32.092222Z","iopub.execute_input":"2023-06-21T19:47:32.092643Z","iopub.status.idle":"2023-06-21T19:47:32.098519Z","shell.execute_reply.started":"2023-06-21T19:47:32.092609Z","shell.execute_reply":"2023-06-21T19:47:32.096712Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"2ba6e2a1-c8ab-49aa-881e-9fc8aceee1b9","_cell_guid":"f1aaf718-1def-46dd-9aec-05272630b961","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"e77557ae-b38b-4567-9019-f075f2399a6d","_cell_guid":"4be15c67-a28d-4e30-98ab-9f7395212554","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}